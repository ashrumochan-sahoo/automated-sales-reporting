# ðŸ›’ Automated Sales Reporting Pipeline

![Python](https://img.shields.io/badge/Python-3.9-blue)
![SQLite](https://img.shields.io/badge/Database-SQLite-green)
![Pandas](https://img.shields.io/badge/Pandas-2.0.3-yellow)
![Tests](https://img.shields.io/badge/Tests-17%20Passing-brightgreen)
![Status](https://img.shields.io/badge/Status-Complete-success)

A production-ready batch ETL pipeline that processes raw retail
sales data into a structured SQLite data warehouse for business
intelligence reporting and analysis.

---

## ðŸ“Š Project Overview

This pipeline demonstrates end-to-end data engineering by:
- **Extracting** raw Superstore sales data from CSV
- **Transforming** it with data quality checks and business logic
- **Loading** it into a star schema SQLite data warehouse
- **Analyzing** it with 10 SQL business intelligence queries

---

## ðŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV File  â”‚â”€â”€â”€â–¶â”‚   Extract   â”‚â”€â”€â”€â–¶â”‚  Transform  â”‚â”€â”€â”€â–¶â”‚    Load     â”‚
â”‚  9,994 rows â”‚    â”‚  validate   â”‚    â”‚  clean data â”‚    â”‚  SQLite DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚   Star Schema DWH   â”‚
                                                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                                    â”‚  â”‚  fact_sales   â”‚  â”‚
                                                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                    â”‚    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”‚
                                                    â”‚  dims     dims      â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚  Analytics Queries  â”‚
                                                    â”‚  10 BI Insights     â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ› ï¸ Tech Stack

| Tool | Version | Purpose |
|------|---------|---------|
| Python | 3.9 | ETL orchestration |
| SQLite | Built-in | Data warehouse |
| Pandas | 2.0.3 | Data transformation |
| SQLAlchemy | 2.0.23 | Database connectivity |
| Pandera | 0.17.2 | Data validation |
| Pytest | 7.4.3 | Unit testing |

---

## ðŸ“ Project Structure
```
automated-sales-reporting/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Source CSV files
â”‚   â”‚   â””â”€â”€ sales_data.csv        # Superstore dataset (9,994 rows)
â”‚   â””â”€â”€ processed/                # Intermediate outputs
â”‚       â”œâ”€â”€ raw_backup.csv        # Raw data backup
â”‚       â””â”€â”€ transformed_sales.csv # Cleaned dataset
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py               # Package initializer
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”œâ”€â”€ extract.py                # Data extraction module
â”‚   â”œâ”€â”€ transform.py              # Data transformation logic
â”‚   â”œâ”€â”€ load.py                   # Database loading module
â”‚   â””â”€â”€ main.py                   # Pipeline orchestration
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql         # Star schema definition
â”‚   â””â”€â”€ analytics_queries.sql    # 10 BI queries
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.sh           # Automation script
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py          # 17 unit tests
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log              # Execution logs
â”œâ”€â”€ .env.example                  # Environment config template
â”œâ”€â”€ pytest.ini                    # Test configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # Project documentation
```

---

## ðŸ“ˆ Dataset

| Property | Value |
|----------|-------|
| Source | Superstore Sales Dataset |
| Records | 9,994 rows |
| Columns | 20 fields |
| Date Range | 2019 - 2022 |
| Total Revenue | $2,297,200 |
| Total Profit | $286,397 |

---

## ðŸ—„ï¸ Database Schema (Star Schema)
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_date   â”‚
                    â”‚  date_key PK â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_customer â”‚    â”‚  fact_sales  â”‚    â”‚  dim_product â”‚
â”‚customer_key  â”‚â—€â”€â”€â”€â”‚  sales_key   â”‚â”€â”€â”€â–¶â”‚  product_key â”‚
â”‚customer_id   â”‚    â”‚  order_id    â”‚    â”‚  product_id  â”‚
â”‚segment       â”‚    â”‚  sales_amt   â”‚    â”‚  category    â”‚
â”‚region        â”‚    â”‚  profit      â”‚    â”‚  sub_cat     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  quantity    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  discount    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ dim_shipping â”‚           â”‚
â”‚shipping_key  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ship_mode     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ ETL Workflow

### Extract Phase
- Reads raw CSV with proper data types
- Validates file existence and structure
- Checks all 20 required columns present
- Logs extraction metrics and null values
- Creates raw backup for auditing

### Transform Phase
- Standardizes column names (lowercase + underscores)
- Removes 1 duplicate row detected
- Converts dates to datetime format
- Converts numerics to correct types
- Creates 9 derived columns:
  - `profit_margin` - profit as % of sales
  - `delivery_days` - days from order to ship
  - `order_year`, `order_month`, `order_quarter`
  - `order_month_name`, `order_day_name`
  - `order_week`, `is_weekend`
- Validates data quality (nulls, ranges, types)

### Load Phase
- Creates star schema in SQLite
- Loads staging table (all 9,993 rows)
- Populates 4 dimension tables
- Loads fact table with foreign keys
- Validates row counts post-load

---

## ðŸ“Š Analytics Queries (10 Total)

| # | Query | Business Question |
|---|-------|------------------|
| 1 | Regional Performance | Which regions drive most revenue? |
| 2 | Top 10 Products | Best performing products? |
| 3 | Monthly Trends | How are sales trending? |
| 4 | Customer Segments | Which segments are most valuable? |
| 5 | Discount Impact | How do discounts affect profit? |
| 6 | Category Performance | Which categories drive revenue? |
| 7 | Shipping Analysis | Does shipping speed affect value? |
| 8 | State Performance | Which states are top markets? |
| 9 | Quarterly Summary | Quarter over quarter comparison? |
| 10 | Pipeline Validation | Data loaded correctly? |

---

## ðŸš€ Setup Instructions

### Prerequisites
- Python 3.9+
- pip
- Git

### Installation

**1. Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/automated-sales-reporting.git
cd automated-sales-reporting
```

**2. Create virtual environment**
```bash
python3 -m venv venv
source venv/bin/activate
```

**3. Install dependencies**
```bash
pip install -r requirements.txt
```

**4. Configure environment**
```bash
cp .env.example .env
```

**5. Add dataset**
```bash
# Place sales_data.csv in data/raw/
cp /path/to/sales_data.csv data/raw/sales_data.csv
```

**6. Run the pipeline**
```bash
./scripts/run_pipeline.sh
```

---

## ðŸ§ª Running Tests
```bash
# Run all tests
pytest -v

# Run with coverage
pytest --cov=pipeline -v
```

**Test Results:**
```
17 passed in 3.2s
- TestExtract:    4 tests âœ…
- TestTransform: 10 tests âœ…
- TestConfig:     3 tests âœ…
```

---

## ðŸ“‹ Pipeline Results

After running the pipeline:

| Table | Rows | Description |
|-------|------|-------------|
| staging_raw_sales | 9,993 | Raw data landing zone |
| dim_date | 1,432 | Unique dates |
| dim_customer | 793 | Unique customers |
| dim_product | 1,862 | Unique products |
| dim_shipping | 4 | Shipping modes |
| fact_sales | 9,993 | Sales transactions |

---

## ðŸ’¡ Key Findings from Analytics

- **West region** drives highest revenue
- **Technology** category has best profit margins
- **Standard Class** shipping is most used
- **Consumer segment** generates most orders
- **High discounts (>30%)** lead to negative profits

---

## ðŸŽ¯ Skills Demonstrated

- ETL pipeline development
- Dimensional data modeling (Star Schema)
- Data quality and validation
- SQL analytics and business intelligence
- Python best practices (modular design)
- Unit testing with pytest
- Shell scripting for automation
- Version control with Git
- Production-ready logging

---

## ðŸ‘¤ Author

**Ashrumochan Sahoo**
Data Engineer | Building data pipelines for business insights

---

*Built as part of data engineering portfolio*